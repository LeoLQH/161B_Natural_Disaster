{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning\n",
        "\n",
        "The year was collected to be from 1923 to 2023.\n",
        "\n",
        "We first would chose only the desired columns to work with before cleaning.\n",
        "To clean it, first need to find what needs to be cleaned."
      ],
      "metadata": {
        "id": "JMUndak9CRsc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gxH6oWDCJ-y",
        "outputId": "c2f951d7-783b-4664-8a8b-15c5436cbc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Raw disaster dataset - Sheet1.csv', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the data\n",
        "print(data.head(150))\n",
        "print(data.columns)"
      ],
      "metadata": {
        "id": "pqFpGwM0Emaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chosing to keep only some columns\n",
        "columns = ['Year','Disaster Group', 'Disaster Subgroup',\n",
        "       'Disaster Type', 'Disaster Subtype', 'Disaster Subsubtype', 'Country', 'ISO', 'Location',\n",
        "       'Origin', 'Associated Dis', 'Associated Dis2', 'Dis Mag Value',\n",
        "       'Dis Mag Scale', 'Start Year', 'Start Month', 'Start Day', 'End Year', 'End Month',\n",
        "       'End Day', 'Number of Days', 'Total Deaths', 'No Injured',\n",
        "       'No Affected', 'No Homeless', 'Total Affected',\n",
        "       'Reconstruction Costs (\\'000 US$)',\n",
        "       'Insured Damages, Adjusted (\\'000 US$)',\n",
        "       'Total Damages, Adjusted (\\'000 US$)']\n",
        "\n",
        "filtered = data[columns]\n",
        "data = filtered\n",
        "\n",
        "# See which column has any null\n",
        "for col in columns:\n",
        "  print(col, \":\" , data[col].isnull().any())"
      ],
      "metadata": {
        "id": "BTu3T8ivM3Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count how many missing for the important columns (total dmg adjusted, start month. start day, end yer, end month, end day)\n",
        "--> Result: start month, start day, end month, end day all has missing values\n",
        "\n",
        "As Affected is too vaguely defined, we'd skip the use of it completely (teh scale of hreporting this is inconsistent across areas), thus no reliable insights could be derived from them.\n",
        "\n",
        "Some columns are allowed to be null, but some are not, dropping all null records in those that are not (total damage and start year can't be null)\n",
        "\n",
        "Dropping na in 2 columns: Total Damages, Adjusted (\\'000 US$), Start Month\n",
        "\n",
        "\n",
        "---\n",
        "*Do not keep location as it is to hard to deal with.*\n",
        "\n",
        "---\n",
        "\n",
        "Keep reconstruction cost, insured , no homeless, no injured, no death, associated disasters, dis value, dis mag scale, origin, diaaster group, subgroup, type, subtype, subsubtype, year.\n",
        "\n",
        "Only keeping the adjusted ones.\n",
        "\n",
        "Using Python to extract those data and handle missing data"
      ],
      "metadata": {
        "id": "-7s6NX8lkVEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "rvCGGWlCq1Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep = ['Year','Disaster Group', 'Disaster Subgroup',\n",
        "       'Disaster Type', 'Disaster Subtype', 'Disaster Subsubtype',\n",
        "       'Origin', 'Associated Dis', 'Associated Dis2', 'Dis Mag Value',\n",
        "       'Dis Mag Scale', 'Start Year', 'Start Month', 'Start Day', 'End Year', 'End Month',\n",
        "       'End Day', 'Number of Days', 'Total Deaths', 'No Injured',\n",
        "       'No Homeless', 'Total Damages, Adjusted (\\'000 US$)']\n",
        "\n",
        "# condition - accpeted years after 1979 only\n",
        "year_cond = data['Year'] > 1979\n",
        "\n",
        "# condition - cleaning missing value data in days and month\n",
        "\n",
        "data = data[keep]\n",
        "data = data[year_cond]\n",
        "data = data.dropna(subset = ['Start Month', 'Start Day', 'End Month', 'End Day',  'Total Damages, Adjusted (\\'000 US$)'])"
      ],
      "metadata": {
        "id": "HHSunt7Goaph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "28lSVKRexdIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll only interested in analyzing Meteorological, Hydrological and Climatological data so we'll only keep those"
      ],
      "metadata": {
        "id": "frjAp8jIy6uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# condition on disaster sub-group\n",
        "cond1 = ['Meteorological', 'Hydrological', 'Climatological']\n",
        "data = data[data['Disaster Subgroup'].isin(cond1)]"
      ],
      "metadata": {
        "id": "Z7gF-6S_yrfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering only those three subtypes, disaster magnitudes became excessively empty. We get a look at it and decide to remove it too."
      ],
      "metadata": {
        "id": "4Qvz2UOT2MrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Dis Mag Value'].info()"
      ],
      "metadata": {
        "id": "WJH6Rhv62Lv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only 189 non-null values out of 588. We can figure out how to fill it in, but for now the practice is to make it binary. (compare between yes there was measurement vs no there was not any measurement)"
      ],
      "metadata": {
        "id": "hTp0GYUu3ZEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making another column as the binary of the Dis Mag Value column\n",
        "\n",
        "data['Dis Mag Value_binary'] = data['Dis Mag Value'].notnull().astype(int)"
      ],
      "metadata": {
        "id": "yBfHJjh63o_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other attributes, including total death, number injured, number homeless have many null values too. However, on the basis that not all disaster must have casualties, we would need to consider it more throroughly to decide on a handling procedure."
      ],
      "metadata": {
        "id": "wQHHHih2wI-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, preliminary cleaning process is done."
      ],
      "metadata": {
        "id": "6d5fiw2_wkgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEED FURTHER CLEANING OF DATA - TOO FEW LANDSLIDE AND EXTREME TEMPERATURE AND DROUGHT."
      ],
      "metadata": {
        "id": "3GGojydm5d0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('Disaster Type').count()"
      ],
      "metadata": {
        "id": "T_8K9iwG5zHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['Disaster Type'].isin(['Storm', 'Flood', 'Wildfire'])]"
      ],
      "metadata": {
        "id": "NoUzTgAH6Dro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are technically done with cleaning the dataset. Now we would start doing some analysis and trasnform the data if need to."
      ],
      "metadata": {
        "id": "V46KHbbFDRXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As total damage is too skewed (left skewed) and having too many outliers, using natural log to rescale the shape\n",
        "# Downside is now we are analyzing not on the total dmg adj but the log of it\n",
        "data[\"Log of Total Damages, Adjusted ('000 US$)\"] = np.log(data[\"Total Damages, Adjusted ('000 US$)\"])"
      ],
      "metadata": {
        "id": "GlFp07JyAhSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the transformation, there are still outliers in the Storm category of Disaster Type.\n",
        "\n",
        "--> ***keep*** the outliers, analyzing them on a one-to-one basis.\n"
      ],
      "metadata": {
        "id": "9ij5AJSNDO6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Many nulls in Associated Disaster 1 and 2. Thus make a new column - binary of the Associated disaster (1 for yes and 0 for no)\n",
        "\n",
        "data['Asso_Dist1_Binary'] = data['Associated Dis'].notnull().astype(int)\n",
        "data['Asso_Dist2_Binary'] = data['Associated Dis2'].notnull().astype(int)\n",
        "\n",
        "# Sums up the 2 binary cols for asso_dist -> get the number of associated disasters\n",
        "data['Number_of_asso_dist'] = data['Asso_Dist1_Binary'] + data['Asso_Dist2_Binary']\n"
      ],
      "metadata": {
        "id": "ZWbWq0ABR1VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total death, number injured and number homeless - hard to differentiate if there is a lack of records or the nulls are just 0, so best practice is to turn it into binary."
      ],
      "metadata": {
        "id": "fnEmZROdg5Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making new columns as the binary of the total death, no injured and no homeless columns\n",
        "\n",
        "data['Total_Deaths_binary'] = data['Total Deaths'].notnull().astype(int)\n",
        "data['No_Injured_binary'] = data['No Injured'].notnull().astype(int)\n",
        "data['No_Homeless_binary'] = data['No Homeless'].notnull().astype(int)"
      ],
      "metadata": {
        "id": "ZTcuoHWvM8nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['No Injured'].info()\n",
        "data['No Homeless'].info()\n",
        "data.groupby('Disaster Type').count()"
      ],
      "metadata": {
        "id": "aWM2ZsjZQ3mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('cleaned_data.csv', index = False)"
      ],
      "metadata": {
        "id": "B77uRXJ81oSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}